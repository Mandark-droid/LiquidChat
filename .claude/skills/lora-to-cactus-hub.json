{
  "name": "lora-to-cactus-hub",
  "title": "LoRA Merge + Cactus Conversion + Hub Push",
  "description": "Merge trained LoRA adapters into the base model, convert to Cactus binary format for mobile deployment, and push to Hugging Face Hub. Cactus is the native runtime for LiquidChat on Android.",
  "category": "model-conversion",
  "prompt": "You are an expert in converting LoRA-trained models to Cactus format for mobile deployment in LiquidChat.\n\nWhen invoked, take action immediately — install Cactus, run conversion, and push to Hub.\n\n## WHAT IS CACTUS FORMAT\n\nCactus is a custom optimized binary format (CACT header + quantized tensors) designed for efficient on-device inference:\n- **INT4**: 50% storage reduction, fastest inference, slight quality trade-off\n- **INT8**: Balanced default — minimal quality loss, good speed\n- **FP16**: Full precision, larger size\n- Supports SIMD-friendly ARM interleaving for mobile CPUs\n- Native runtime for LiquidChat Android app via `cactus-react-native`\n\n## PREREQUISITE: COMPLETED TRAINING\n\nBefore running this skill, you need a trained LoRA adapter on Hub from `/unsloth-jobs-training`.\nFor LiquidChat: `kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora`\n\n## STEP 1: INSTALL CACTUS\n\n```bash\npip install cactus-compute\n```\n\nVerify:\n```bash\ncactus --version\n```\n\n## STEP 2: CONVERT WITH LORA MERGE (single command)\n\nThe `cactus convert` command handles the full pipeline: download base model → load LoRA → merge → quantize → serialize:\n\n```bash\n# Merge LoRA into base and convert to Cactus INT8 (recommended default)\ncactus convert LiquidAI/LFM2.5-1.2B-Instruct ./converted-liquidchat \\\n  --lora kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora\n\n# Or specify INT4 for smaller size\ncactus convert LiquidAI/LFM2.5-1.2B-Instruct ./converted-liquidchat \\\n  --lora kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora \\\n  --precision INT4\n```\n\n## STEP 3: VERIFY CONVERTED MODEL\n\n```bash\n# Test on desktop (interactive chat)\ncactus run ./converted-liquidchat\n\n# Or run a specific prompt\ncactus run ./converted-liquidchat --prompt \"Send a text to John saying I'll be late\"\n```\n\nExpected output: JSON function call like:\n```json\nsend_message(recipient=\"John\", message=\"I'll be late\")\n```\n\n## STEP 4: PUSH TO HUB\n\n```python\nfrom huggingface_hub import HfApi\nimport os\n\napi = HfApi()\n\n# Create repo if needed\napi.create_repo(\n    repo_id=\"kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora-cactus\",\n    repo_type=\"model\",\n    exist_ok=True\n)\n\n# Upload converted files\napi.upload_folder(\n    folder_path=\"./converted-liquidchat\",\n    repo_id=\"kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora-cactus\",\n    repo_type=\"model\",\n    commit_message=\"Add Cactus-converted LiquidChat LoRA weights\"\n)\n\nprint(\"Model available at: https://huggingface.co/kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora-cactus\")\n```\n\n## CONVERTED OUTPUT STRUCTURE\n\n```\nconverted-liquidchat/\n├── weights/\n│   └── <model_name>/\n│       ├── *.bin          # Quantized weight tensors (CACT binary format)\n│       └── config.txt     # Model metadata\n├── vocab.txt              # Vocabulary\n├── merges.txt             # BPE merges\n├── special_tokens.json    # eos, pad, bos, tool tokens\n├── tokenizer_config.txt   # Tokenizer settings\n└── chat_template.jinja2   # ChatML template\n```\n\n## ALTERNATIVE: CONVERT ON HF JOBS (if no local GPU)\n\nIf the local machine is slow, run conversion on HF Jobs:\n\n```bash\nhf jobs uv run \\\n  --flavor a10g-large \\\n  --timeout 30m \\\n  --secrets HF_TOKEN \\\n  -- python -c \"\nimport subprocess, os\nsubprocess.run(['pip', 'install', 'cactus-compute'], check=True)\nsubprocess.run([\n  'cactus', 'convert', 'LiquidAI/LFM2.5-1.2B-Instruct', '/tmp/converted',\n  '--lora', 'kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora'\n], check=True)\nfrom huggingface_hub import HfApi\nHfApi().upload_folder(\n  folder_path='/tmp/converted',\n  repo_id='kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora-cactus',\n  repo_type='model'\n)\n\"\n```\n\n## PRECISION SELECTION GUIDE\n\n| Precision | Size Reduction | Quality | Use Case |\n|-----------|---------------|---------|----------|\n| INT4 | ~75% | Slight loss | Battery-constrained devices, RAM < 4GB |\n| INT8 | ~50% | Minimal loss | Default recommendation for LiquidChat |\n| FP16 | 0% | Lossless | Desktop testing, high-end devices |\n\nFor LiquidChat Android (≥3GB RAM devices): **INT8 recommended**\n\n## QUALITY VERIFICATION\n\nAfter conversion, Cactus reports:\n- **MSE** (Mean Squared Error) — lower is better\n- **SNR** (Signal-to-Noise Ratio, dB) — higher is better\n- **Cosine Similarity** — >0.99 is excellent\n\nIf quality metrics are poor, try FP16 or INT8 instead of INT4.\n\n## PERFORMANCE TARGETS FOR LIQUIDCHAT\n\n| Device | INT8 Speed |\n|--------|------------|\n| Flagship Android (Snapdragon 8 Gen) | 20-35 tokens/sec |\n| Mid-range Android | 10-20 tokens/sec |\n| Budget Android | 8-13 tokens/sec |\n\n## TROUBLESHOOTING\n\n| Error | Fix |\n|-------|-----|\n| `cactus: command not found` | Run `pip install cactus-compute` first |\n| `Architecture not supported` | Check model arch in supported list (LFM2 supported) |\n| LoRA load error | Verify adapter repo is public or HF_TOKEN is set |\n| Hub upload fails | Check token write access: `hf whoami` |\n| Out of memory during merge | Use HF Jobs approach above instead |\n\n## STEP-BY-STEP FLOW WHEN INVOKED\n\n1. Confirm base model and LoRA adapter IDs\n2. Install cactus-compute: `pip install cactus-compute`\n3. Run conversion with `--lora` flag\n4. Test with `cactus run` on a sample prompt\n5. Push to Hub with clear naming (`-cactus` suffix)\n6. Report Hub URL for use in `/build-apk-liquidchat`\n\n## NAMING CONVENTION\n\n```\nBase: LiquidAI/LFM2.5-1.2B-Instruct\nLoRA: kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora\nCactus: kshitijthakkar/LFM2.5-1.2B-Instruct-liquidchat-lora-cactus\n```\n",
  "examples": [
    "Convert my trained LoRA adapter to Cactus format",
    "Merge LFM2.5 LoRA and convert for mobile deployment",
    "Push Cactus weights to Hub for LiquidChat",
    "Run the full LoRA merge and Cactus conversion pipeline"
  ]
}
